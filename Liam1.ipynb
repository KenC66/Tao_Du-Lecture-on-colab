{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from torchvision import datasets\n","from torchvision.transforms import ToTensor\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.autograd import Variable\n","import platform, psutil, time\n","from pynvml import *"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_data = datasets.MNIST(\n","    root = 'data',\n","    train = True,                         \n","    transform = ToTensor(), \n","    download = True)\n","test_data = datasets.MNIST(\n","    root = 'data', \n","    train = False, \n","    transform = ToTensor()\n",")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["isualisation<br>\n","Plot one train data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import matplotlib.pyplot as plt\n","i=10000\n","plt.imshow(train_data.data[i], cmap='gray')\n","plt.title('%i' % train_data.targets[i])\n","#plt.show()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["lot multiple"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["figure = plt.figure(figsize=(10, 8))\n","cols, rows = 5, 5\n","for i in range(1, cols * rows + 1):\n","    sample_idx = torch.randint(len(train_data), size=(1,)).item()\n","    img, label = train_data[sample_idx]\n","    figure.add_subplot(rows, cols, i)\n","    plt.title(label)\n","    plt.axis(\"off\")\n","    plt.imshow(img.squeeze(), cmap=\"gray\")\n","#plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from torch.utils.data import DataLoader\n","loaders = {\n","    'train' : torch.utils.data.DataLoader(train_data, \n","                                          batch_size=100, \n","                                          shuffle=True, \n","                                          num_workers=1),\n","    \n","    'test'  : torch.utils.data.DataLoader(test_data, \n","                                          batch_size=100, \n","                                          shuffle=True, \n","                                          num_workers=1),\n","}\n","#loaders"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        self.hidden1 = nn.Linear(28*28,512)\n","        self.relu = nn.ReLU()\n","        self.hidden2 = nn.Linear(512,10)\n","        self.softmax = nn.Softmax(dim=0)\n","        \n","# [batch_size, 28, 28]\n","    def forward(self, x):\n","        x = x.view(x.size(0), -1)  # flatten the input (batch_size, 28* 28)\n","        x = self.hidden1(x)\n","        x = self.relu(x)\n","        x = self.hidden2(x)\n","        x = self.softmax(x)\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def train1(num_epochs, model, loaders):\n","    \n","    model.train()\n","        \n","    # Train the model\n","    total_step = len(loaders['train'])\n","    print('\\ttrain1 for %d epochs, %d sets of data  each with batch size = %d' % \n","          (num_epochs,total_step,loaders['train'].batch_size))\n","    for epoch in range(num_epochs):\n","        print ('Epoch %d/%d [Model 1] Loss =' % (epoch + 1, num_epochs),end= \" \")  \n","        for i, (images, labels) in enumerate(loaders['train']):\n","            \n","            # gives batch data, normalize x when iterate train_loader\n","            b_x = Variable(images)   # batch x\n","            b_y = Variable(labels)   # batch y\n","            output = model(b_x)#[0]    \n","            \n","            #print(b_x.shape)\n","            #print(output.shape)\n","            #print(b_y.shape)\n","            loss = loss_func(output, b_y)\n","            \n","            # clear gradients for this training step   \n","            optimizer.zero_grad()           \n","            \n","            # backpropagation, compute gradients \n","            loss.backward()                # apply gradients             \n","            optimizer.step()                \n","            \n","            if (i+1) % 100 == 0:\n","                       print(f' {loss.item():.3f} at {i+1:d}', end=\"\")               \n","        print(f\"/{total_step:d} end of epoch={epoch+1:d}\")\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def test(model, epos, my_id):\n","    model.eval()\n","    test_loss = 0\n","    correct = 0\n","    i = 0\n","    with torch.no_grad():\n","        for data, target in loaders['test']:\n","            output = model(data)             #print(data.shape, output.shape, target.shape)1\n","            test_loss += F.nll_loss(output, target) #).item()\n","            pred = output.data.max(1, keepdim=True)[1]\n","            correct += pred.eq(target.data.view_as(pred)).sum()\n","            if i==0:\n","                fig,ax=plt.subplots(1,2)\n","                ax[0].imshow( data[0,0,:,:] )\n","                ax[0].set_title('GT %d Pred %d' % (target[0],pred[0]))\n","                ax[1].imshow( data[1,0,:,:] )\n","                ax[1].set_title('GT %d Pred %d' % (target[1],pred[1]))\n","                fig.suptitle('First two images in Batch 0 [%d epochs]' % epos)\n","            i += 1\n","        test_loss /= len(loaders['test'].dataset)\n","        per = 100. * correct / len(loaders['test'].dataset)\n","        print('\\nModel {:d} Test set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n","          my_id,test_loss, correct, len(loaders['test'].dataset),   per))\n","        fig.suptitle('Model %d First two images in Batch 0 [%d epochs] Accu: %.2f' % (my_id, epos, per))\n","    plt.savefig('Pred_by_model_%d.jpg' % my_id)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class CNN(nn.Module):  # MODEL 2\n","    def __init__(self):\n","        super(CNN, self).__init__()\n","# Input to conv1 will be image of shape [batch_size,1,28,28] (height and width are 28 for this example)\n","        self.conv1 = nn.Sequential(   \n","            nn.Conv2d(in_channels=1,out_channels=10,kernel_size=(3,3),padding=1), #output of this conv is of shape [BS,10,28,28]\n","            nn.ReLU(), \n","            nn.MaxPool2d(kernel_size=(2,2)) #output of this is [BS,10,14,14]\n","        )\n","        self.conv2 = nn.Sequential( \n","            nn.Conv2d(in_channels=10,out_channels=20,kernel_size=(3,3),padding=1), #output of this is [BS,20,14,14]\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size=(2,2)) # output of this is [BS,20,7,7]\n","        )\n","        self.conv3 = nn.Sequential(\n","            nn.Conv2d(in_channels=20,out_channels=30,kernel_size=(3,3),padding=1), #Output of this [BS,30,7,7]\n","            nn.ReLU(),\n","            nn.Conv2d(in_channels=30,out_channels=30,kernel_size=(3,3),padding=1),\n","            nn.ReLU()\n","        )\n","        self.conv4 = nn.Sequential(\n","            nn.Conv2d(in_channels=30,out_channels=20,kernel_size=(3,3),padding=1),\n","            nn.ReLU(),\n","            nn.Upsample(scale_factor=2))\n","        \n","        self.conv5 = nn.Sequential(\n","            nn.Conv2d(in_channels=20,out_channels=10,kernel_size=(3,3),padding=1),\n","            nn.ReLU(),\n","            nn.Upsample(scale_factor=2)) #[BS,10,28,28]\n","        \n","        \n","        self.conv6 = nn.Sequential(\n","            nn.Conv2d(in_channels=10,out_channels=1,kernel_size=(1,1)),\n","            nn.Flatten(1),\n","            nn.ReLU(),  ##\n","            nn.Linear(28*28,10),\n","            nn.Sigmoid()            \n","        )\n","            \n","    def forward(self, x,plot=False):\n","        x1 = self.conv1(x)\n","        x2 = self.conv2(x1)\n","        x3 = self.conv3(x2)\n","        x4 = self.conv4(x3)\n","        x5 = self.conv5(x4)\n","        x6 = self.conv6(x5)\n"," \n","        if plot:\n","            print('Input shape', x.shape)\n","            print('After layer 1', x1.shape)\n","            print('After layer 2', x2.shape)\n","            print('After layer 3', x3.shape)\n","            print('After layer 4', x4.shape)\n","            print('After layer 5', x5.shape)\n","            print('After layer 6', x6.shape)\n","        return x6"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def train2(num_epochs, model, loaders):\n","    \n","    model.train()\n","        \n","    # Train the model\n","    total_step = len(loaders['train'])\n","    print('\\ttrain2 for %d epochs, %d sets of data  each with batch size = %d' % \n","          (num_epochs,total_step,loaders['train'].batch_size))    \n","    for epoch in range(num_epochs):\n","        print ('Epoch %d/%d [Model 2] Loss =' % (epoch + 1, num_epochs),end= \" \")  \n","        for i, (images, labels) in enumerate(loaders['train']):\n","            # images is of size [batch_size, 28, 28]\n","            \n","            # gives batch data, normalize x when iterate train_loader\n","            b_x = Variable(images)   # batch x\n","            b_y = Variable(labels)   # batch y\n","            output = model(b_x)#[0]               print(i, b_y.shape, output.shape)\n","            loss = loss_func(output, b_y)\n","            \n","            # clear gradients for this training step   \n","            optimizer.zero_grad()           \n","            \n","            # backpropagation, compute gradients \n","            loss.backward()                # apply gradients             \n","            optimizer.step()                \n","            \n","            if (i+1) % 100 == 0:\n","                print(f' {loss.item():.4f} at {i+1:d}', end=\"\")               \n","        print(f\"/{total_step:d} end of epoch={epoch+1:d}\")\n","                        "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["if __name__ == '__main__':\n","    print('info1:', train_data, end =\"\")\n","    print(' info2:', train_data.data.size(), end =\"\")\n","    print(' and info3: ', train_data.targets.size())\n","    start_time = time.time()\n","    use_cuda = torch.cuda.is_available()\n","    device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n","    s24=1014**2\n","    print('\\nRunning: Liam1.ipynb on', device, '=', platform.node())\n","    model1 = Net()  # Case 1: Full Connected (dense matrix)\n","    model2 = CNN()  # Case 2: Convolutional  (sparse matrix)\n","    \n","    Which = input(\"Choose model to train (test) 1 for FCN or 2 for CNN : \")\n","    ID = int(  Which )\n","# Train:\n","    num_epochs = 8\n","    if ID==1:\n","        loss_func = nn.NLLLoss()   \n","        model = model1\n","        learning_rate = 0.01\n","        optimizer = optim.Adam(model1.parameters(), lr = learning_rate)   \n","        train1(num_epochs, model, loaders)\n","    else:\n","        model = model2\n","        learning_rate = 0.001\n","        optimizer = optim.Adam(model2.parameters(), lr = learning_rate)   \n","        loss_func = nn.CrossEntropyLoss()\n","        train2(num_epochs, model, loaders)\n","    loss_func;     optimizer\n","    end_time = time.time()\n","    if use_cuda:\n","        nvmlInit()\n","        h = nvmlDeviceGetHandleByIndex(0);    info = nvmlDeviceGetMemoryInfo(h)\n","        mem = f'Free {info.free/s24:.1f} MB (out of {info.total/s24:.1f} MB)'\n","    else:\n","        free = int(psutil.virtual_memory().total - psutil.virtual_memory().available)\n","        tot = int(psutil.virtual_memory().total)\n","        mem = f'Free {free/s24:.1f} MB (out of {tot/s24:.1f} MB)'\n","    print('\\tMemory usage:',mem, ' and  Train Time used = %.2f seconds' % (end_time-start_time) )\n","    #print(model)\n","    # im,label = train_data[0]\n","    # im = im.unsqueeze(1)\n","    # print(im.shape)\n","    #  output = model2(im,plot=True)\n","\n","    # test to see the prediction\n","    test(model, num_epochs,ID)\n","    plt.pause(5)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":2}
